# uplas-ai-agents/nlp_content_agent/main.py
from fastapi import FastAPI, HTTPException, BackgroundTasks, status
from pydantic import BaseModel, Field, validator
from typing import List, Dict, Optional, Any, Union
import os
import uuid
import time
import httpx # For any potential future internal calls (not primary for this agent)
import logging
import json
import random # For mock generation

# GCP Clients - Import moved inside __init__ for VertexAILLMClientForNLP if needed
# from google.cloud import aiplatform

SUPPORTED_LANGUAGES = ["en-US", "fr-FR", "es-ES", "de-DE", "pt-BR", "zh-CN", "hi-IN"] 
DEFAULT_LANGUAGE = "en-US" 


# --- Configuration ---
GCP_PROJECT_ID = os.getenv("GCP_PROJECT_ID") 
GCP_LOCATION = os.getenv("GCP_LOCATION", "us-central1") 
NLP_LLM_MODEL_NAME = os.getenv("NLP_LLM_MODEL_NAME", "gemini-1.5-flash-001") 

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO) 
logger = logging.getLogger(__name__) 

# --- Pydantic Models for NLP Agent Output ---

class NlpTopic(BaseModel): 
    topic_id: str = Field(default_factory=lambda: f"topic_{uuid.uuid4().hex[:8]}", examples=["topic_superposition_intro"])
    topic_title: str = Field(..., examples=["Understanding Superposition"])
    key_concepts: List[str] = Field(default_factory=list, examples=[
        "Qubits can represent 0, 1, or a combination of both.",
        "Superposition allows quantum computers to perform many calculations at once."
    ])
    content_with_tags: str = Field(..., examples=["A classical bit is either 0 or 1. <analogy type=\"comparison_to_classical_needed\" for_concept=\"bit_vs_qubit\" context_keywords=\"classical_bit,qubit,superposition\" /> ... <difficulty type=\"foundational_info\" />"])
    estimated_complexity_score: Optional[float] = Field(None, ge=0.0, le=1.0, description="Normalized complexity score (0=easiest, 1=hardest)")
    suggested_prerequisites: Optional[List[str]] = Field(default_factory=list, description="List of prerequisite topic names or concept keywords.")


class NlpLesson(BaseModel): 
    lesson_id: str = Field(default_factory=lambda: f"lesson_{uuid.uuid4().hex[:8]}", examples=["lesson_quantum_basics"])
    lesson_title: str = Field(..., examples=["What is a Qubit?"])
    lesson_summary: Optional[str] = Field(None, description="A brief summary of the lesson, generated by LLM.")
    topics: List[NlpTopic] = Field(default_factory=list)


class ProcessedModule(BaseModel): 
    module_id: str = Field(..., examples=["course101_module3_processed"]) 
    source_module_id: str = Field(..., examples=["course101_module3_raw"]) 
    module_title: Optional[str] = Field(None, examples=["Introduction to Quantum Computing"])
    language_code: str = Field(..., examples=["en-US"])
    lessons: List[NlpLesson] = Field(default_factory=list)
    processing_time_ms: Optional[float] = None
    llm_model_used: Optional[str] = None
    module_level_visual_aid_summary: Optional[List[Dict[str,str]]] = Field(default_factory=list, description="Summary of all visual aid suggestions in the module.")

# --- Pydantic Models for API Request ---

class ProcessContentRequest(BaseModel): 
    module_id: str = Field(..., examples=["course101_module3_raw"], description="Unique ID for the raw course module being processed.")
    raw_text_content: str = Field(..., min_length=50, description="The full raw text content of the course module.") 
    language_code: str = Field(DEFAULT_LANGUAGE, examples=SUPPORTED_LANGUAGES)
    module_title: Optional[str] = Field(None, description="Optional title for the module if known.")

    @validator('language_code')
    def validate_language_code(cls, v): 
        if v not in SUPPORTED_LANGUAGES:
            logger.warning(f"NovaSpark Warning: Unsupported language_code '{v}' in ProcessContentRequest. Falling back to default '{DEFAULT_LANGUAGE}'.")
            return DEFAULT_LANGUAGE
        return v

# --- Vertex AI LLM Client Logic (NovaSpark Enhanced for Specificity) ---
class VertexAILLMClientForNLP: 
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.is_initialized = False # NovaSpark: Track initialization
        if not GCP_PROJECT_ID: 
            logger.error("NovaSpark Critical: GCP_PROJECT_ID not set. Vertex AI LLM client for NLP WILL NOT FUNCTION.")
        else:
            try:
                from google.cloud import aiplatform # Moved import here
                aiplatform.init(project=GCP_PROJECT_ID, location=GCP_LOCATION)
                self.is_initialized = True
                logger.info(f"NovaSpark: Vertex AI SDK initialized for NLP Agent. Project: {GCP_PROJECT_ID}, Location: {GCP_LOCATION}")
            except Exception as e_init:
                logger.error(f"NovaSpark Critical: Failed to initialize Vertex AI SDK for NLP Agent. Error: {e_init}", exc_info=True)

    async def _call_gemini_api(self, system_prompt: str, user_query: str, is_json_output: bool = True, 
                               pydantic_model_for_schema: Optional[Any] = None) -> str: 
        if not self.is_initialized:
            logger.error("NovaSpark Error: Vertex AI Client for NLP was not initialized. Cannot call Gemini API.")
            # In a real scenario, might raise an exception or return an error state
            # that translates to an HTTP 503 from the endpoint.
            # For mock, we can proceed if specific conditions met, otherwise return error JSON.
            if "MOCK_FORCE_SUCCESS" in os.environ: # Allow forcing mock success for testing without init
                 pass
            else:
                return json.dumps({"error": "NLP LLM Client not initialized."})


        from vertexai.generative_models import GenerativeModel, Part, GenerationConfig, HarmCategory, HarmBlockThreshold # Moved import

        logger.info(f"NovaSpark: Calling Gemini API for NLP. Model: {self.model_name}. JSON Output: {is_json_output}")
        # logger.debug(f"NovaSpark NLP System Prompt (sample): {system_prompt[:350]}...")
        # logger.debug(f"NovaSpark NLP User Query (sample): {user_query[:350]}...")

        # --- NovaSpark: Actual call to Gemini API (conceptual structure) ---
        # This part would be uncommented and fully implemented by Mugambi.
        # For now, it falls through to mocks.
        #
        # try:
        #     model = GenerativeModel(self.model_name, system_instruction=[Part.from_text(system_prompt)])
        #     gen_config_params = {
        #         "temperature": 0.2, # Lower temp for deterministic structuring
        #         "max_output_tokens": 8192, 
        #         "top_p": 0.9, "top_k": 35 
        #     }
        #     if is_json_output:
        #         gen_config_params["response_mime_type"] = "application/json"
        #         if pydantic_model_for_schema:
        #             try:
        #                 gen_config_params["response_schema"] = pydantic_model_for_schema.model_json_schema()
        #                 logger.info(f"NovaSpark: Using Pydantic schema for {pydantic_model_for_schema.__name__} in Gemini call.")
        #             except Exception as e_schema:
        #                 logger.error(f"NovaSpark Error creating schema for {pydantic_model_for_schema.__name__}: {e_schema}")
        #     
        #     generation_config = GenerationConfig(**gen_config_params)
        #     safety_settings = {
        #         HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        #         HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        #         HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        #         HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        #     }
        #
        #     api_response = await model.generate_content_async(
        #         [Part.from_text(user_query)],
        #         generation_config=generation_config,
        #         safety_settings=safety_settings
        #     )
        #     # TODO: Add token count logging from api_response.usage_metadata
        #     if api_response.candidates and api_response.candidates[0].content.parts:
        #         response_text = "".join([part.text for part in api_response.candidates[0].content.parts if part.text])
        #         logger.info(f"NovaSpark: Received LLM response (length: {len(response_text)} chars).")
        #         return response_text
        #     else:
        #         logger.error("NovaSpark Error: No content in LLM response or no candidates.")
        #         return json.dumps({"error": "LLM returned no content."}) # Ensure JSON string for error
        #
        # except Exception as e_gemini:
        #     logger.error(f"NovaSpark Error during Gemini API call: {e_gemini}", exc_info=True)
        #     return json.dumps({"error": f"Gemini API call failed: {str(e_gemini)}"})
        #
        # --- END Actual Call ---


        # Mocked responses for NovaSpark framework (Task 5 Enhanced Mock):
        if "Your objectives are to structure and enrich this content" in system_prompt and "Divide this lesson snippet into logical topics" in system_prompt:
            logger.info("NovaSpark Mock: Generating ENHANCED mock response for micro_segment_and_enrich_lesson.")
            # Determine language from prompt for mock generation:
            lang_match = re.search(r"language: \[([a-zA-Z]{2,3}-[a-zA-Z]{2,4})\]", system_prompt)
            mock_lang = lang_match.group(1) if lang_match else DEFAULT_LANGUAGE
            
            topic_titles_map = {
                "en-US": ["Introduction to Specificity", "Actionable Tag Examples"],
                "fr-FR": ["Introduction à la Spécificité", "Exemples de Balises Actionnables"],
                # Add other languages as needed for diverse mocking
            }
            key_concepts_map = {
                "en-US": [["Tag Attributes", "Contextual Keywords", "Actionability"], ["Downstream Agent Use", "Personalization Priming"]],
                "fr-FR": [["Attributs de Balise", "Mots-clés Contextuels", "Actionnabilité"], ["Utilisation par Agent Aval", "Amorçage de Personnalisation"]],
            }
            content_snippets_map = {
                "en-US": [
                    "Specificity in NLP tags is crucial. <analogy type=\"user_profile_driven_analogy_needed\" for_concept=\"tag_specificity\" context_keywords=\"NLP_tags,actionable_data,downstream_processing\" suggested_theme_if_obvious=\"data_pipelines\" /> This allows for better AI Tutor personalization. <visual_aid_suggestion type=\"flowchart\" description=\"Flow from NLP tag generation to AI Tutor using the tag's context_keywords and for_concept attributes to personalize an analogy.\" keywords=\"NLP_agent,AI_Tutor,personalization,tag_attributes,workflow\" complexity=\"moderate\" purpose=\"illustrate_process\" for_text_segment_id=\"nlp_spec_seg_001\" /> This is <difficulty type=\"intermediate_detail\">important</difficulty>.",
                    "For example, an <example domain=\"user_profile_driven_example_needed\" for_concept=\"actionable_visual_aid_tag\" context_keywords=\"visual_aid_suggestion,diagram_generation,specificity\" suggested_domain_if_obvious=\"automated_report_graphics\" /> helps the TTV agent. This section is <difficulty type=\"foundational_info\">fundamental</difficulty>. Any questions? <interactive_question_opportunity text_suggestion=\"How can specific keywords in tags improve AI responses?\" />"
                ],
                "fr-FR": [
                    "La spécificité des balises NLP est cruciale. <analogy type=\"user_profile_driven_analogy_needed\" for_concept=\"specificite_balise\" context_keywords=\"balises_NLP,donnees_actionnables,traitement_aval\" suggested_theme_if_obvious=\"pipelines_de_donnees\" /> Cela permet une meilleure personnalisation par le Tuteur IA. <visual_aid_suggestion type=\"flowchart\" description=\"Flux de la génération de balises NLP au Tuteur IA utilisant les attributs context_keywords et for_concept de la balise pour personnaliser une analogie.\" keywords=\"agent_NLP,Tuteur_IA,personnalisation,attributs_balise,workflow\" complexity=\"moderate\" purpose=\"illustrate_process\" for_text_segment_id=\"nlp_spec_seg_001_fr\" /> C'est <difficulty type=\"intermediate_detail\">important</difficulty>.",
                    "Par exemple, un <example domain=\"user_profile_driven_example_needed\" for_concept=\"balise_aide_visuelle_actionnable\" context_keywords=\"suggestion_aide_visuelle,generation_diagramme,specificite\" suggested_domain_if_obvious=\"graphiques_rapport_automatises\" /> aide l'agent TTV. Cette section est <difficulty type=\"foundational_info\">fondamentale</difficulty>. Des questions? <interactive_question_opportunity text_suggestion=\"Comment des mots-clés spécifiques dans les balises peuvent-ils améliorer les réponses de l'IA?\" />"
                ],
            }
            
            mock_lesson_detail = {
              "lesson_title": f"Enriched Lesson on Tag Specificity ({mock_lang})", # Using lang from prompt
              "lesson_summary": f"This lesson details how to create highly specific and actionable enrichment tags for Uplas AI agents, in {mock_lang}.",
              "topics": [
                  {
                      "topic_id": f"nlp_specificity_intro_{uuid.uuid4().hex[:4]}",
                      "topic_title": topic_titles_map.get(mock_lang, topic_titles_map["en-US"])[0],
                      "key_concepts": key_concepts_map.get(mock_lang, key_concepts_map["en-US"])[0],
                      "content_with_tags": content_snippets_map.get(mock_lang, content_snippets_map["en-US"])[0],
                      "estimated_complexity_score": 0.7,
                      "suggested_prerequisites": ["Basic NLP Tagging"]
                  },
                  {
                      "topic_id": f"nlp_actionable_examples_{uuid.uuid4().hex[:4]}",
                      "topic_title": topic_titles_map.get(mock_lang, topic_titles_map["en-US"])[1],
                      "key_concepts": key_concepts_map.get(mock_lang, key_concepts_map["en-US"])[1],
                      "content_with_tags": content_snippets_map.get(mock_lang, content_snippets_map["en-US"])[1],
                      "estimated_complexity_score": 0.5
                  }
              ]
            }
            return json.dumps(mock_lesson_detail)

        elif "segment it into distinct, high-level lessons" in system_prompt: 
            logger.info("NovaSpark Mock: Generating standard mock response for macro_segment_module.")
            mock_lessons_data = [
                {"lesson_title": "Lesson 1: The Grand Introduction to Mocking", "text_segment_start_index": 0, "text_segment_end_index": 150},
                {"lesson_title": "Lesson 2: Deep Dive into Advanced Mocking", "text_segment_start_index": 151, "text_segment_end_index": 300}
            ]
            return json.dumps(mock_lessons_data)
        
        logger.warning("NovaSpark Mock: No specific mock response matched in _call_gemini_api for NLP based on system prompt.")
        return json.dumps({"warning": "No specific mock matched", "system_prompt_start": system_prompt[:100]})


    async def macro_segment_module(self, full_text: str, language_code: str, module_title: Optional[str]) -> List[Dict[str, Any]]: 
        system_prompt = ( 
            "You are an expert instructional designer specializing in modular content creation. "
            f"Your task is to analyze the provided course module text, which is in [{language_code}], "
            f"and segment it into distinct, coherent, high-level lessons. Each lesson should represent a significant unit of learning."
            f"For each identified lesson, provide a concise and engaging 'lesson_title' in [{language_code}]. "
            "Also, provide the exact 'text_segment_start_index' (integer, 0-based) and 'text_segment_end_index' (integer, exclusive) "
            "from the original text that constitutes that lesson. "
            f"The overall module is titled: '{module_title if module_title else 'Not Provided'}'. "
            "Focus on logical flow, clear topic transitions, and appropriate lesson lengths for online learning. "
            "Respond ONLY with a valid JSON list of objects. Each object in the list MUST have three keys: "
            "'lesson_title' (string), 'text_segment_start_index' (integer), and 'text_segment_end_index' (integer)."
        )
        user_query = f"Here is the full module text in [{language_code}] to segment into lessons:\n\n---MODULE TEXT START---\n{full_text}\n---MODULE TEXT END---"

        # NovaSpark: Define Pydantic model for expected list structure for validation if using response_schema with Gemini
        class MacroSegmentItem(BaseModel):
            lesson_title: str
            text_segment_start_index: int
            text_segment_end_index: int
        class MacroSegmentList(BaseModel):
            segments: List[MacroSegmentItem] # If LLM can wrap it in a root key

        response_str = "{}"
        try:
            # For macro-segmentation, direct JSON list is fine, Pydantic schema might be overkill for Gemini's direct list output.
            response_str = await self._call_gemini_api(system_prompt, user_query, is_json_output=True)
            lessons_data = json.loads(response_str)
            if not isinstance(lessons_data, list): 
                logger.error(f"NovaSpark Error: LLM response for macro-segmentation was not a list. Response: {response_str[:300]}")
                raise ValueError("LLM response for macro-segmentation was not a list.")
            for item in lessons_data: # Validate each item
                if not all(k in item for k in ("lesson_title", "text_segment_start_index", "text_segment_end_index")):
                    logger.error(f"NovaSpark Error: Invalid item in macro-segmentation list: {item}. Missing required keys.")
                    # Optionally, filter out invalid items instead of raising immediately
                    raise ValueError("Invalid item structure in LLM response for macro-segmentation.")
                if not (isinstance(item["text_segment_start_index"], int) and isinstance(item["text_segment_end_index"], int) and item["text_segment_start_index"] <= item["text_segment_end_index"]):
                    logger.error(f"NovaSpark Error: Invalid indices in macro-segmentation item: {item}.")
                    raise ValueError("Invalid indices in LLM response for macro-segmentation.")
            return lessons_data
        except json.JSONDecodeError as e: 
            logger.error(f"NovaSpark JSONDecodeError in macro_segment_module: {e}. Response: {response_str[:500]}", exc_info=True)
            raise ValueError(f"Failed to parse LLM response for macro-segmentation: {e}")
        except Exception as e: 
            logger.error(f"NovaSpark Error in macro_segment_module: {e}", exc_info=True)
            raise

    # NovaSpark Enhanced: `micro_segment_and_enrich_lesson` with a more specific prompt (Task 5)
    async def micro_segment_and_enrich_lesson(self, lesson_text_snippet: str, lesson_title_from_macro: str, language_code: str) -> Dict[str, Any]: 
        # This is the core prompt refined for Task 5
        system_prompt = (
            f"You are an AI pedagogical content specialist for Uplas EdTech. Your task is to meticulously analyze the lesson text snippet provided (lesson title: '{lesson_title_from_macro}', language: [{language_code}]) and restructure it into a detailed JSON object. "
            "Your objectives are to structure and enrich this content:"
            f"1. Generate a concise 'lesson_summary' (1-2 sentences) for this snippet, IN [{language_code}]."
            "2. Divide the snippet into logical, granular 'topics'. For each topic:"
            "   a. 'topic_id': Generate a unique, url-friendly ID (e.g., 't1_concept_x')."
            f"  b. 'topic_title': Create a clear, descriptive title IN [{language_code}]."
            f"  c. 'key_concepts': Extract 2-4 crucial key concepts as a list of short strings, IN [{language_code}]."
            "   d. 'content_with_tags': This is the original text for the topic, but you MUST intelligently intersperse it with XML-like semantic tags where pedagogically appropriate. ALL TEXT generated within tag attributes (like descriptions or suggestions) MUST also be IN [{language_code}]."
            "      Focus on creating HIGHLY SPECIFIC and ACTIONABLE tags:"
            "      - `<analogy type=\"user_profile_driven_analogy_needed\" for_concept=\"[specific_concept_being_explained_by_analogy]\" context_keywords=\"[comma_separated_keywords_from_text_segment_related_to_concept]\" suggested_theme_if_obvious=\"[e.g.,_finance,_sports_if_text_implies_a_theme]\" />` "
            "        (Use this when an analogy would clarify a complex idea. The AI Tutor will use these attributes and the user's profile to generate the final analogy. Ensure `for_concept` and `context_keywords` are very specific to the text section where the tag is placed.)"
            "      - `<example domain=\"user_profile_driven_example_needed\" for_concept=\"[specific_concept_being_illustrated]\" context_keywords=\"[comma_separated_keywords_from_text_segment]\" suggested_domain_if_obvious=\"[e.g.,_software_testing,_medical_diagnosis_if_text_implies]\" />` "
            "        (Use for illustrating points. Ensure `for_concept` and `context_keywords` are specific. The AI Tutor will use these to generate a personalized example.)"
            "      - `<interactive_question_opportunity text_suggestion=\"[Suggest_a_brief_checking_question_in_{language_code}_here]\" />` (At points good for engagement/reflection. The question itself should be IN [{language_code}].)"
            "      - `<visual_aid_suggestion type=\"[diagram|chart|animation_cue|image_idea|code_snippet_highlight]\" description=\"[Detailed_description_of_visual_IN_{language_code}]\" keywords=\"[comma_separated_keywords_from_text_relevant_to_visual_IN_{language_code}]\" complexity=\"[simple|moderate|detailed]\" purpose=\"[illustrate_process|compare_data|show_structure|engagement]\" for_text_segment_id=\"[lesson_title_slugified]_[topic_index]_[tag_index]\" />` "
            "        (For concepts best shown visually. Make the `description` and `keywords` very specific and actionable. `for_text_segment_id` should be unique within the lesson.)"
            "      - `<difficulty type=\"[foundational_info|intermediate_detail|advanced_detail]\" />` (To classify content complexity.)"
            "   e. Optionally: 'estimated_complexity_score' (float, 0.0-1.0) and 'suggested_prerequisites' (list of strings IN [{language_code}])."
            f"All text content you generate for any field (titles, concepts, summaries, tag attributes like descriptions or suggestions) MUST be IN [{language_code}]."
            "Respond ONLY with a single, valid JSON object. The root object should have 'lesson_title' (string, use the provided '{lesson_title_from_macro}'), 'lesson_summary' (string), and 'topics' (a list of topic objects as described above)."
        )
        user_query = f"Analyze and enrich this lesson text snippet, which is part of lesson '{lesson_title_from_macro}', in [{language_code}]:\n\n---LESSON SNIPPET START---\n{lesson_text_snippet}\n---LESSON SNIPPET END---"

        # NovaSpark: Define Pydantic model for the expected root structure of the LLM's JSON response for this task
        class MicroSegmentTopic(BaseModel): # Corresponds to NlpTopic but for direct LLM output validation
            topic_id: str
            topic_title: str
            key_concepts: List[str]
            content_with_tags: str
            estimated_complexity_score: Optional[float] = None
            suggested_prerequisites: Optional[List[str]] = None
        class MicroSegmentRoot(BaseModel):
            lesson_title: str
            lesson_summary: str
            topics: List[MicroSegmentTopic]
        
        response_str = "{}"
        try:
            response_str = await self._call_gemini_api(system_prompt, user_query, is_json_output=True, pydantic_model_for_schema=MicroSegmentRoot)
            enriched_lesson_data = json.loads(response_str)
            
            # Validate with Pydantic model after parsing JSON
            validated_data = MicroSegmentRoot(**enriched_lesson_data) # This will raise ValidationError if malformed
            logger.info(f"NovaSpark: Successfully parsed and validated micro-segmentation response for '{lesson_title_from_macro}'.")

            # NovaSpark: Further checks can be done here if needed, e.g., all generated text fields are in target lang (harder to auto-validate)
            for topic in validated_data.topics:
                if not all(k_attr in topic.model_fields for k_attr in ("topic_id", "topic_title", "key_concepts", "content_with_tags")): # Check against Pydantic model fields
                     logger.warning(f"NovaSpark Warning: A topic in micro-segmentation is missing required keys: {topic.topic_id if hasattr(topic, 'topic_id') else 'N/A'}")
            return validated_data.model_dump() # Return as dict
            
        except json.JSONDecodeError as e: 
            logger.error(f"NovaSpark JSONDecodeError in micro_segment_and_enrich_lesson: {e}. Response: {response_str[:500]}", exc_info=True)
            raise ValueError(f"Failed to parse LLM response for micro-segmentation: {e}")
        except Exception as e: # Catches Pydantic's ValidationError and other exceptions
            logger.error(f"NovaSpark Error in micro_segment_and_enrich_lesson (Validation or other): {e}. Response: {response_str[:500]}", exc_info=True)
            raise ValueError(f"LLM response validation or other error in micro-segmentation: {e}")


nlp_llm_client = VertexAILLMClientForNLP(model_name=NLP_LLM_MODEL_NAME) 

app = FastAPI( 
    title="Uplas NLP Content Structuring & Augmentation Agent - NovaSpark Specificity Edition",
    description="Processes raw course content into structured, richly tagged, and actionable learning units using Vertex AI, with NovaSpark's specificity enhancements.",
    version="0.3.0" # Incremented for Task 5
)

@app.post("/v1/process-course-content", response_model=ProcessedModule, status_code=status.HTTP_200_OK) 
async def process_content_endpoint(request_data: ProcessContentRequest, background_tasks: BackgroundTasks): 
    start_time = time.perf_counter()
    logger.info(f"NovaSpark: Received request to process module_id: {request_data.module_id} in language: {request_data.language_code}")

    if not GCP_PROJECT_ID or not nlp_llm_client.is_initialized: 
        logger.error("NovaSpark Critical: NLP service cannot operate (GCP_PROJECT_ID missing or LLM client not initialized).")
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail="NLP service is not properly configured.")

    final_lessons_processed: List[NlpLesson] = []
    # all_visual_aid_suggestions_module_level: List[Dict[str,str]] = [] # Logic for this not yet implemented

    try:
        logger.info(f"NovaSpark Stage 1: Starting macro-segmentation for module: {request_data.module_id}")
        macro_segments = await nlp_llm_client.macro_segment_module( 
            request_data.raw_text_content,
            request_data.language_code,
            request_data.module_title
        )
        logger.info(f"NovaSpark Stage 1: Macro-segmentation complete. Found {len(macro_segments)} potential lessons.")

        for i, segment_info in enumerate(macro_segments): 
            lesson_title_from_macro = segment_info.get("lesson_title", f"Lesson {i+1} (Title N/A)")
            start_idx = segment_info.get("text_segment_start_index")
            end_idx = segment_info.get("text_segment_end_index")

            if start_idx is None or end_idx is None or not isinstance(start_idx, int) or not isinstance(end_idx, int) or start_idx >= end_idx or end_idx > len(request_data.raw_text_content) : 
                logger.warning(f"NovaSpark Warning: Skipping invalid segment (bad indices: {start_idx}-{end_idx}, text_len: {len(request_data.raw_text_content)}): {segment_info} for module {request_data.module_id}")
                continue

            lesson_text_snippet = request_data.raw_text_content[start_idx:end_idx]
            if not lesson_text_snippet.strip(): 
                logger.info(f"NovaSpark Info: Skipping empty lesson text snippet for lesson: {lesson_title_from_macro}")
                continue

            logger.info(f"NovaSpark Stage 2: Starting micro-segmentation for lesson: '{lesson_title_from_macro}'")
            enriched_lesson_data_dict = await nlp_llm_client.micro_segment_and_enrich_lesson( 
                lesson_text_snippet,
                lesson_title_from_macro,
                request_data.language_code
            ) # This now returns a dict
            
            current_lesson_topics_processed: List[NlpTopic] = []
            for topic_data_dict in enriched_lesson_data_dict.get("topics", []): 
                try:
                    # NovaSpark: Create NlpTopic from the dict structure returned by micro_segment_and_enrich_lesson
                    processed_topic = NlpTopic(**topic_data_dict) # Pydantic validation happens here
                    current_lesson_topics_processed.append(processed_topic)
                except Exception as e_topic_val: # Catch Pydantic validation error or others
                    logger.warning(f"NovaSpark Warning: Failed to validate/process a topic from LLM output: {topic_data_dict.get('topic_id', 'N/A')}. Error: {e_topic_val}", exc_info=True)
            
            final_lesson_title = enriched_lesson_data_dict.get("lesson_title", lesson_title_from_macro) 
            lesson_summary_from_llm = enriched_lesson_data_dict.get("lesson_summary")

            if current_lesson_topics_processed: # Only add lesson if it has topics
                final_lessons_processed.append(NlpLesson( 
                    lesson_title=final_lesson_title,
                    lesson_summary=lesson_summary_from_llm,
                    topics=current_lesson_topics_processed
                ))
                logger.info(f"NovaSpark Stage 2: Micro-segmentation for lesson '{final_lesson_title}' complete. {len(current_lesson_topics_processed)} topics processed.")
            else:
                logger.warning(f"NovaSpark Stage 2: No topics processed for lesson '{final_lesson_title}'. Skipping lesson.")


    except ValueError as ve: 
        logger.error(f"NovaSpark ValueError during content processing for {request_data.module_id}: {ve}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Error processing content: {str(ve)}")
    except Exception as e: 
        logger.error(f"NovaSpark Unexpected error during content processing for {request_data.module_id}: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"An unexpected error occurred during content processing: {str(e)}")

    if not final_lessons_processed: 
        logger.warning(f"NovaSpark Warning: No lessons were successfully processed for module: {request_data.module_id}. This might indicate issues with content or LLM responses.")
        # Return empty list of lessons but still 200 OK, or raise error? For now, 200 OK with empty.

    end_time = time.perf_counter()
    processing_time_ms = (end_time - start_time) * 1000 

    logger.info(f"NovaSpark: Successfully processed module_id: {request_data.module_id}. Time taken: {processing_time_ms:.2f} ms. Generated {len(final_lessons_processed)} lessons.")
    return ProcessedModule( 
        source_module_id=request_data.module_id,
        module_title=request_data.module_title,
        language_code=request_data.language_code,
        lessons=final_lessons_processed,
        processing_time_ms=round(processing_time_ms, 2),
        llm_model_used=nlp_llm_client.model_name
    )

# --- Health Check Endpoint ---
@app.get("/health", status_code=status.HTTP_200_OK) 
async def health_check():
    service_name = "NLP_Content_Agent_NovaSpark_Specificity"
    if not GCP_PROJECT_ID or not NLP_LLM_MODEL_NAME: 
        return {"status": "unhealthy", "reason": "Required configurations (GCP_PROJECT_ID, NLP_LLM_MODEL_NAME) are missing.", "service": service_name, "innovate_ai_enhancements_active": True}
    if nlp_llm_client is None or not nlp_llm_client.is_initialized: 
         return {"status": "unhealthy", "reason": "Vertex AI client for NLP not properly initialized.", "service": service_name, "innovate_ai_enhancements_active": True}
    return {"status": "healthy", "service": service_name, "innovate_ai_enhancements_active": True}

if __name__ == "__main__": 
    import uvicorn
    logger.info(f"NovaSpark: Starting {app.title} v{app.version} for local development...")
    if not GCP_PROJECT_ID: 
        print("NovaSpark Warning: GCP_PROJECT_ID is not set. Please set this environment variable for the NLP agent.")
    
    port = int(os.getenv("PORT", 8005)) 
    uvicorn.run(app, host="0.0.0.0", port=port)
